{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
                        "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
                        "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "758586ff702a479da5514cb06a53dea9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "Phi3ForCausalLM(\n",
                            "  (model): Phi3Model(\n",
                            "    (embed_tokens): Embedding(32064, 5120, padding_idx=32000)\n",
                            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
                            "    (layers): ModuleList(\n",
                            "      (0-39): 40 x Phi3DecoderLayer(\n",
                            "        (self_attn): Phi3Attention(\n",
                            "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
                            "          (qkv_proj): Linear(in_features=5120, out_features=7680, bias=False)\n",
                            "          (rotary_emb): Phi3RotaryEmbedding()\n",
                            "        )\n",
                            "        (mlp): Phi3MLP(\n",
                            "          (gate_up_proj): Linear(in_features=5120, out_features=35840, bias=False)\n",
                            "          (down_proj): Linear(in_features=17920, out_features=5120, bias=False)\n",
                            "          (activation_fn): SiLU()\n",
                            "        )\n",
                            "        (input_layernorm): Phi3RMSNorm()\n",
                            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
                            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
                            "        (post_attention_layernorm): Phi3RMSNorm()\n",
                            "      )\n",
                            "    )\n",
                            "    (norm): Phi3RMSNorm()\n",
                            "  )\n",
                            "  (lm_head): Linear(in_features=5120, out_features=32064, bias=False)\n",
                            ")"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load model directly\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "import torch\n",
                "\n",
                "model_path = \"microsoft/Phi-3-medium-4k-instruct\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True)\n",
                "device = \"cuda:0\"\n",
                "model = model.to(device)\n",
                "model = model.half()\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "# Information\n",
                        "{information}\n",
                        "\n",
                        "# Answer\n",
                        "{answer}\n"
                    ]
                }
            ],
            "source": [
                "instruction = '# instruction: Write a text explaining the following fugure.'\n",
                "shot_format = '''\n",
                "# Information\n",
                "{information}\n",
                "\n",
                "# Answer\n",
                "{answer}'''\n",
                "print(shot_format)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import json \n",
                "\n",
                "shots = []\n",
                "shot_file = Path('preprocessed/valid/그래프 기반 문장 생성 말뭉치 2022.jsonl').open()\n",
                "for shot in shot_file.readlines():\n",
                "    line = json.loads(shot)\n",
                "    title = line['metadata']['title']\n",
                "    figure_title = line['metadata']['figure_title']\n",
                "    date = line['metadata']['date']\n",
                "    publisher = line['metadata']['publisher']\n",
                "    figure = line['figure']\n",
                "    \n",
                "    reference = line['str_annotation']['reference_str']\n",
                "    paraphrase = line['str_annotation']['paraphrase_str']\n",
                "    \n",
                "    information = {\n",
                "        'title': title,\n",
                "        'date': date,\n",
                "        'publisher': publisher,\n",
                "        'figure_title': figure_title,\n",
                "        'figure': figure\n",
                "    }\n",
                "    shots.append(shot_format.format(information=information, answer=reference))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "# instruction: Write a text explaining the following fugure.\n",
                        "# Information\n",
                        "{'title': '수입 조제분유, 수입가격 대비 국내 판매가격은 1.8~4.1배', 'date': '2022-02-28', 'publisher': '한국소비자원', 'figure_title': '조제분유 선택 이유', 'figure': {'type': '가로막대그래프', 'x_label': '조제분유 선택 이유', 'y_label': '응답률(%, 500명)', 'x': ['아이가 잘 먹어서', '간편해서(휴대·조제)', '가격 때문에', '안전(제조과정, 유통과정)한 제품일 것 같아서', '영양성분 때문에', '모유와 가장 유사할 것 같아서', '원유가 좋을 것 같아서', '기타'], 'y': [44.4, 4.8, 4, 9.6, 20.6, 6.8, 4.8, 5]}}\n",
                        "\n",
                        "# Answer\n",
                        "이 그래프는 가장 최근 구매한 조제분유 선택 이유에 대해 설문한 것이다. '아이가 잘 먹어서' 해당 조제 분유를 구매하였다는 응답이 44.4%로 가장 높았고, '영양성분'(20.6%), '안전한 제품'(9.6%) 등의 응답이 뒤를 이었다. 반면에 '가격 때문에' 해당 조제 분유를 구매하였다는 응답이 4.0%로 가장 낮았고, '간편해서'(4.8%), '원유가 좋을 것 같아서'(4.8%) 등의 응답이 뒤를 이었다. 수입 조제분유가 수입 가격 대비 국내 판매가격이 1.8~4.1배나 높지만 가격에 의해 구매를 결정하는 소비자가 가장 적은 것을 확인할 수 있다.\n",
                        "\n",
                        "# Information\n",
                        "{'title': '교육 분야 5년(2017~2022) 성과자료집 발간', 'date': '2022-04-21', 'publisher': '교육부', 'figure_title': '고교 무상교육 지원 인원', 'figure': {'type': '세로막대그래프', 'x_label': '연도', 'y_label': '지원 인원(만 명)', 'x': [2019, 2020, 2021], 'y': [44, 85, 124]}}\n",
                        "\n",
                        "# Answer\n",
                        "이 그래프는 2019년부터 2021년까지 고등학교 무상교육 지원 인원을 보여준다. 2019년에는 44만 명으로 가장 낮았으나 2020년에는 85만 명으로 늘어났으며 2021년에는 124만 명으로 가장 많았다. 그래프를 통해 무상교육 지원 추세를 파악할 수 있다.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "prompt = instruction + '\\n'.join(shots) + '\\n'\n",
                "print(prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "train:   0%|          | 0/1 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
                        "train: 100%|██████████| 1/1 [5:13:26<00:00, 18806.30s/it]\n",
                        "valid: 100%|██████████| 1/1 [00:35<00:00, 35.83s/it]\n"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
                        "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
                        "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
                        "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "import re\n",
                "from tqdm import tqdm\n",
                "\n",
                "settings = json.loads(Path('../../../settings.json').read_text())",
                "preprocessed_data_path = Path(settings['preprocessed_data_path'])",
                "data_path = Path('.').resolve()",
                "data_name = data_path.name",
                "source_name = data_path.parent.name",
                "preprocessed_dir = preprocessed_data_path/source_name/data_name",

                "source_data_dir = preprocessed_dir/'preprocessed'\n",
                "splits = ['train', 'valid']\n",
                "tasks = ['그래프 설명 생성']\n",
                "task_data_dir = preprocessed_dir/'preprocessed_task'\n",
                "task_data_dir.mkdir(exist_ok=True)\n",
                "for task in tasks:\n",
                "    task_path = task_data_dir/task\n",
                "    task_path.mkdir(exist_ok=True)\n",
                "    \n",
                "#### prepare for task preprocess\n",
                "\n",
                "\n",
                "\n",
                "#### prepare for task preprocess end\n",
                "\n",
                "#### task preprocess\n",
                "for split in splits:\n",
                "    source_data_dir_split = source_data_dir/split\n",
                "    task_files = [(task_data_dir/task/f'{split}.jsonl').open('w', encoding='utf-8') for task in tasks]\n",
                "    for source_data in tqdm(list(source_data_dir_split.iterdir()), desc=split):\n",
                "        source_data = source_data.open()\n",
                "        for line in source_data.readlines():\n",
                "            line = json.loads(line)\n",
                "            \n",
                "            #### data preprocess\n",
                "            title = line['metadata']['title']\n",
                "            figure_title = line['metadata']['figure_title']\n",
                "            date = line['metadata']['date']\n",
                "            publisher = line['metadata']['publisher']\n",
                "            figure = line['figure']\n",
                "            \n",
                "            reference = line['str_annotation']['reference_str']\n",
                "            paraphrase = line['str_annotation']['paraphrase_str']\n",
                "            \n",
                "            information = {\n",
                "                'title': title,\n",
                "                'date': date,\n",
                "                'publisher': publisher,\n",
                "                'figure_title': figure_title,\n",
                "                'figure': figure\n",
                "            }\n",
                "            prompt_text = prompt+shot_format.format(information=information, answer='')\n",
                "            \n",
                "            inputs = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n",
                "            with torch.no_grad():\n",
                "                output = model.generate(\n",
                "                    inputs,\n",
                "                    max_new_tokens=500, \n",
                "                    num_return_sequences=1,\n",
                "                    do_sample=True,\n",
                "                    top_k=50,\n",
                "                    top_p=0.95,\n",
                "                    temperature=0.7\n",
                "                )\n",
                "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
                "            response = generated_text[len(prompt_text):].split('# Information')[0].strip()\n",
                "            #### data preprocess end \n",
                "\n",
                "\n",
                "            #### 그래프 설명 생성\n",
                "            data = {'input': None, 'output': None}\n",
                "            ## preprocess data from line\n",
                "            data['input'] = information\n",
                "            data['output'] = [reference, paraphrase, response]\n",
                "            ## preprocess data from line end\n",
                "            task_files[0].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "            # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "            #### 그래프 설명 생성 end\n",
                "\n",
                "            \n",
                "    #         break\n",
                "    #     break\n",
                "    # break\n",
                "\n",
                "    for path in task_files:\n",
                "        path.close()      \n",
                "        \n",
                "#### task preprocess end      "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
