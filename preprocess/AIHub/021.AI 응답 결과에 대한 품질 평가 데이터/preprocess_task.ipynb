{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "device = \"cuda\" # the device to load the model onto\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    \"Qwen/Qwen2-72B-Instruct-GPTQ-Int4\", # the quantized model\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B-Instruct-GPTQ-Int4\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "\n",
                "def get_outputs(messages, rejected_text):\n",
                "\n",
                "    PROMPT = '''## Instruction: \\n당신은 친절한 전문가 AI 어시스턴트 입니다. 사용자와 어시스턴트의 대화 히스토리가 제공됩니다. 히스토리의 문맥을 파악하여 적절히 대답하세요. 답변은 두 문장 이내로 대답하세요.\\n'''\n",
                "    \n",
                "    text = PROMPT+'## Dialogue history: \\n'\n",
                "    for message in messages:\n",
                "        text += f\"{message['role']}: {message['content']}\\n\"\n",
                "    \n",
                "    # text += f\"## Wrong Answer : \\nassistant: {rejected_text}\\n\"\n",
                "    # text += \"## Answer : \\nassistant: \"\n",
                "    text += \"## Answer : \"\n",
                "    text += \"assistant: \"\n",
                "    # print(text)\n",
                "    \n",
                "    input_ids = tokenizer.encode(text,return_tensors=\"pt\").to(model.device)\n",
                "    terminators = [\n",
                "        tokenizer.eos_token_id,\n",
                "        # tokenizer.bos_token_id,\n",
                "        # tokenizer.pad_token_id,\n",
                "    ]\n",
                "\n",
                "    outputs = model.generate(\n",
                "        input_ids,\n",
                "        max_new_tokens=256,\n",
                "        eos_token_id=terminators,\n",
                "        num_beams=3,\n",
                "        repetition_penalty=0.5,\n",
                "        do_sample=True,\n",
                "        temperature=0.6,\n",
                "        top_p=0.9\n",
                "    )\n",
                "    outputs = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
                "    outputs = outputs.split(\"##\")[0].split('\\n')[0].strip()\n",
                "    return outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "import re\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "\n",
                "settings = json.loads(Path('../../../settings.json').read_text())",
                "preprocessed_data_path = Path(settings['preprocessed_data_path'])",
                "data_path = Path('.').resolve()",
                "data_name = data_path.name",
                "source_name = data_path.parent.name",
                "preprocessed_dir = preprocessed_data_path/source_name/data_name",

                "source_data_dir = preprocessed_dir/'preprocessed'\n",
                "splits = ['train', 'valid']\n",
                "tasks = ['dialog', 'dpo', '대화 요약']\n",
                "task_data_dir = preprocessed_dir/'preprocessed_task'\n",
                "task_data_dir.mkdir(exist_ok=True)\n",
                "for task in tasks:\n",
                "    task_path = task_data_dir/task\n",
                "    task_path.mkdir(exist_ok=True)\n",
                "    \n",
                "#### prepare for task preprocess\n",
                "speaker_type_map = {\n",
                "    'human' : 'user',\n",
                "    'bot' : 'assistant',\n",
                "}\n",
                "def get_new_eval(eval_dicts):\n",
                "    if len(eval_dicts) == 0:\n",
                "        return True\n",
                "    keys = [\n",
                "        'linguistic_acceptability',\n",
                "        'consistency',\n",
                "        'unbias',\n",
                "        'harmlessness',\n",
                "        'no_hallucination'\n",
                "    ]\n",
                "    \n",
                "    \n",
                "    values = sum([list({k:v for k,v in eval_dict.items() \n",
                "                        if k in keys}.values()) for eval_dict in eval_dicts], [])\n",
                "    count = 0\n",
                "    for value in values:\n",
                "        if value == 'no':\n",
                "            count += 1\n",
                "        if count > 2:\n",
                "            return False\n",
                "    return True\n",
                "\n",
                "#### prepare for task preprocess end\n",
                "with torch.no_grad():\n",
                "#### task preprocess\n",
                "    for split in splits:\n",
                "        source_data_dir_split = source_data_dir/split\n",
                "        task_files = [(task_data_dir/task/f'{split}.jsonl').open('w', encoding='utf-8') for task in tasks]\n",
                "        for source_data in tqdm(list(source_data_dir_split.iterdir()), desc=split):\n",
                "            source_data = source_data.open()\n",
                "            for line in source_data.readlines():\n",
                "                line = json.loads(line)\n",
                "                \n",
                "                #### data preprocess\n",
                "                speakers = {s['id']:speaker_type_map[s['speaker_type']] for s in line['metadata']['speakers']}\n",
                "                messages = [{\n",
                "                                'role':speakers[utt['speaker_id']], \n",
                "                                'content':utt['utterance_text'], \n",
                "                                'eval': get_new_eval(utt['utterance_evaluation'])\n",
                "                            } for utt in line['utterances']]\n",
                "                summary = line['conversation_summary']\n",
                "                #### data preprocess end \n",
                "                \n",
                "                #### dialog\n",
                "                new_messages = []\n",
                "                for message in messages:\n",
                "                    message_eval = message.pop('eval')\n",
                "                    if message['role'] == 'user':\n",
                "                        new_messages.append(message)\n",
                "                    else:\n",
                "                        if message_eval == True:\n",
                "                            new_messages.append(message)\n",
                "                        else:\n",
                "                            rejected_text = message['content']\n",
                "                            new_message = get_outputs(new_messages, rejected_text)\n",
                "                            chosen = {\n",
                "                                'role': speaker_type_map['bot'],\n",
                "                                'content': new_message\n",
                "                            }\n",
                "                            rejected = {\n",
                "                                'role': speaker_type_map['bot'],\n",
                "                                'content': rejected_text\n",
                "                            }\n",
                "                            print(new_messages[-1]['content'])\n",
                "                            print('-',new_message)\n",
                "                            data = {'conversations': new_messages, 'chosen': chosen, 'rejected': rejected}\n",
                "                            task_files[1].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                            # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "                            new_messages.append(chosen)\n",
                "                \n",
                "                data = {'conversations': new_messages}\n",
                "                task_files[0].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "                #### dialog end\n",
                "                \n",
                "                \n",
                "                #### 대화 요약\n",
                "                data = {'input': None, 'output': None}\n",
                "                ## preprocess data from line\n",
                "                data['input'] = new_messages\n",
                "                data['output'] = summary\n",
                "                ## preprocess data from line end\n",
                "                task_files[2].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "                #### 대화 요약 end\n",
                "                \n",
                "                \n",
                "                \n",
                "                # break564\n",
                "            # break\n",
                "        # break\n",
                "\n",
                "        for path in task_files:\n",
                "            path.close()      \n",
                "        \n",
                "#### task preprocess end      "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
