{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "36a4e11c4cf14e4a8d01a17fcff2974a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "Gemma2ForCausalLM(\n",
                            "  (model): Gemma2Model(\n",
                            "    (embed_tokens): Embedding(256000, 4608, padding_idx=0)\n",
                            "    (layers): ModuleList(\n",
                            "      (0-45): 46 x Gemma2DecoderLayer(\n",
                            "        (self_attn): Gemma2Attention(\n",
                            "          (q_proj): Linear(in_features=4608, out_features=4096, bias=False)\n",
                            "          (k_proj): Linear(in_features=4608, out_features=2048, bias=False)\n",
                            "          (v_proj): Linear(in_features=4608, out_features=2048, bias=False)\n",
                            "          (o_proj): Linear(in_features=4096, out_features=4608, bias=False)\n",
                            "          (rotary_emb): Gemma2RotaryEmbedding()\n",
                            "        )\n",
                            "        (mlp): Gemma2MLP(\n",
                            "          (gate_proj): Linear(in_features=4608, out_features=36864, bias=False)\n",
                            "          (up_proj): Linear(in_features=4608, out_features=36864, bias=False)\n",
                            "          (down_proj): Linear(in_features=36864, out_features=4608, bias=False)\n",
                            "          (act_fn): PytorchGELUTanh()\n",
                            "        )\n",
                            "        (input_layernorm): Gemma2RMSNorm()\n",
                            "        (post_attention_layernorm): Gemma2RMSNorm()\n",
                            "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
                            "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
                            "      )\n",
                            "    )\n",
                            "    (norm): Gemma2RMSNorm()\n",
                            "  )\n",
                            "  (lm_head): Linear(in_features=4608, out_features=256000, bias=False)\n",
                            ")"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "import torch\n",
                "\n",
                "model_id_en = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
                "model_id_zh = \"shenzhi-wang/Llama3-8B-Chinese-Chat\"\n",
                "model_id = \"google/gemma-2-27b-it\"\n",
                "\n",
                "tokenizer_en = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
                "model_en = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map={'':0},\n",
                ")\n",
                "tokenizer_zh=tokenizer_en\n",
                "model_zh=model_en\n",
                "# tokenizer_zh = AutoTokenizer.from_pretrained(model_id_zh, padding_side='left')\n",
                "# model_zh = AutoModelForCausalLM.from_pretrained(\n",
                "#     model_id_zh,\n",
                "#     torch_dtype=torch.bfloat16,\n",
                "#     device_map={'':0},\n",
                "# )\n",
                "model_en.eval()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer_en.pad_token_id = model_en.config.eos_token_id\n",
                "tokenizer_zh.pad_token_id = model_zh.config.eos_token_id"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "train:   0%|          | 0/38 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0420 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0421 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0091 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0110 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0051 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0054 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0367 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0262 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0297 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0181 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0270 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0271 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0277 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0279 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0280 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0018 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0015 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0060 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0195 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0411 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0237 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0065 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0072 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0012 ko-en\n",
                        "preprocessed/train/TL2.jsonl 방송_애니메이션_0011 ko-en\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "train:   3%|▎         | 1/38 [03:44<2:18:26, 224.50s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1213 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0866 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1257 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0936 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0083 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0855 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0853 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1331 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1138 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1139 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1135 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0524 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1328 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0995 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0740 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1214 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0926 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1271 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1162 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1206 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1122 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1099 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0403 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0576 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0981 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0112 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0439 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0161 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0651 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0809 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1046 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1315 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1317 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0091 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0795 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0864 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0925 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0987 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0362 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0401 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0797 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1313 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0288 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0580 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0585 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0701 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0043 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0700 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0974 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0969 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0986 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0985 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1225 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1307 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1323 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_1100 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0012 ko-en\n",
                        "preprocessed/train/TL3.jsonl 방송_영화드라마_0030 ko-en\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "train:   3%|▎         | 1/38 [14:40<9:02:42, 880.06s/it]\n"
                    ]
                },
                {
                    "ename": "OutOfMemoryError",
                    "evalue": "CUDA out of memory. Tried to allocate 5.70 GiB. GPU 0 has a total capacity of 79.15 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 77.11 GiB memory in use. Of the allocated memory 70.56 GiB is allocated by PyTorch, and 6.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[3], line 153\u001b[0m\n\u001b[1;32m    150\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m judge_tokenizer(input_samples, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(judge_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mjudge_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjudge_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#[0][\"generated_text\"]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m judges \u001b[38;5;241m=\u001b[39m judge_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    159\u001b[0m judges_only \u001b[38;5;241m=\u001b[39m [judge\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJudge:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m judge \u001b[38;5;129;01min\u001b[39;00m judges]\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/utils.py:1989\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1982\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1983\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1984\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1985\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1986\u001b[0m     )\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1989\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1997\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2004\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2005\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m     )\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/utils.py:2932\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2929\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2931\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2932\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2935\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:944\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    941\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    943\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:784\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    773\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    774\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    775\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m         cache_position,\n\u001b[1;32m    782\u001b[0m     )\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:526\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    523\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    536\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:253\u001b[0m, in \u001b[0;36mGemma2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    250\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m causal_mask\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    254\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    255\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n",
                        "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/functional.py:1860\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.70 GiB. GPU 0 has a total capacity of 79.15 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 77.11 GiB memory in use. Of the allocated memory 70.56 GiB is allocated by PyTorch, and 6.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "import re\n",
                "from tqdm import tqdm\n",
                "from torch.utils.data import Dataset\n",
                "\n",
                "settings = json.loads(Path('../../../settings.json').read_text())",
                "preprocessed_data_path = Path(settings['preprocessed_data_path'])",
                "data_path = Path('.').resolve()",
                "data_name = data_path.name",
                "source_name = data_path.parent.name",
                "preprocessed_dir = preprocessed_data_path/source_name/data_name",

                "source_data_dir = preprocessed_dir/'preprocessed'\n",
                "splits = ['train', 'valid']\n",
                "tasks = ['LM(한국어)', 'LM(영어)', 'LM(중국어)', '번역(한-영)', '번역(한-중)', '번역(영-한)', '번역(중-한)']\n",
                "task_data_dir = preprocessed_dir/'preprocessed_task'\n",
                "task_data_dir.mkdir(exist_ok=True)\n",
                "for task in tasks:\n",
                "    task_path = task_data_dir/task\n",
                "    task_path.mkdir(exist_ok=True)\n",
                "    \n",
                "#### prepare for task preprocess\n",
                "sentence_set_kr = set()\n",
                "sentence_set_en = set()\n",
                "sentence_set_zh = set()\n",
                "regex_pattern = re.compile(r'&[a-z]*&')\n",
                "input_prompt_text_en = '''You are a translation classifier. Given two sentences, judge whether the translation was successful with yes or no. Here are some samples:\n",
                "Source: 인터넷에서 검색하니 지금 대원을 모집하고 있어요.\n",
                "Target: When I search on the Internet, it's recruiting members.\n",
                "Judge: Yes\n",
                "    \n",
                "Source: 음, 나는 바르셀로나는 무조건 추천한다.\n",
                "Target: Okay, now shall we add the eclair cream in advance?\n",
                "Judge: No\n",
                "\n",
                "Source: {input}\n",
                "Target: {output}\n",
                "Judge:'''\n",
                "\n",
                "input_prompt_text_zh = '''你是一个翻译分类器。给定两个句子,判断翻译是否成功,用\"是\"或\"否\"来回答。 以下是一些示例。:\n",
                "Source: 인터넷에서 검색하니 지금 대원을 모집하고 있어요.\n",
                "Target: 我在网上查了一下，他们现在正在招募会员。\n",
                "Judge: Yes\n",
                "    \n",
                "Source: 음, 나는 바르셀로나는 무조건 추천한다.\n",
                "Target: 好的，现在我们要提前添加闪电泡芙奶油吗？\n",
                "Judge: No\n",
                "\n",
                "Source: {input}\n",
                "Target: {output}\n",
                "Judge:'''\n",
                "\n",
                "#### prepare for task preprocess end\n",
                "\n",
                "topic_dialog = {}\n",
                "#### task preprocess\n",
                "for split in splits:\n",
                "    source_data_dir_split = source_data_dir/split\n",
                "    task_files = [(task_data_dir/task/f'{split}.jsonl').open('w', encoding='utf-8') for task in tasks]\n",
                "    for source_data in tqdm(list(sorted(list(source_data_dir_split.iterdir()),\n",
                "                                        key=lambda x : int(str(x).split('L')[1].split('.')[0]))), desc=split):\n",
                "        source_data = source_data.open()\n",
                "        for line in source_data.readlines():\n",
                "            line = json.loads(line)\n",
                "            \n",
                "            #### data preprocess\n",
                "            \n",
                "            \n",
                "            #### data preprocess end \n",
                "\n",
                "            #### LM(한국어)\n",
                "            if line['typeInfo']['language'] == '한국어':\n",
                "                data = {'text': None}\n",
                "                ## preprocess data from line\n",
                "                for utt in line['dialogs']:\n",
                "                    if 'text' not in utt:\n",
                "                        continue\n",
                "                    text = utt['text']\n",
                "                    if text in sentence_set_kr:\n",
                "                        continue\n",
                "                    sentence_set_kr.add(text)\n",
                "                    data['text'] = text\n",
                "                    ## preprocess data from line end\n",
                "                    task_files[0].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                    # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "            #### LM(한국어) end\n",
                "            \n",
                "            \n",
                "            #### LM(영어)\n",
                "            if line['typeInfo']['language'] == '영어':\n",
                "                data = {'text': None}\n",
                "                ## preprocess data from line\n",
                "                for utt in line['dialogs']:\n",
                "                    if 'text' not in utt:\n",
                "                        continue\n",
                "                    text = utt['text']\n",
                "                    if text in sentence_set_en:\n",
                "                        continue\n",
                "                    sentence_set_en.add(text)\n",
                "                    data['text'] = text\n",
                "                    ## preprocess data from line end\n",
                "                    task_files[1].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                    # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "            #### LM(영어) end\n",
                "            \n",
                "            \n",
                "            #### LM(중국어)\n",
                "            if line['typeInfo']['language'] == '중국어':\n",
                "                data = {'text': None}\n",
                "                ## preprocess data from line\n",
                "                for utt in line['dialogs']:\n",
                "                    if 'text' not in utt:\n",
                "                        continue\n",
                "                    text = utt['text']\n",
                "                    if text in sentence_set_en:\n",
                "                        continue\n",
                "                    sentence_set_en.add(text)\n",
                "                    data['text'] = text\n",
                "                    ## preprocess data from line end\n",
                "                    task_files[2].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                    # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "            #### LM(중국어) end\n",
                "            \n",
                "            topic = line['typeInfo']['topic'] + ' ' + line['typeInfo']['language_pair']\n",
                "            dialog_language = line['typeInfo']['language']\n",
                "            if topic not in topic_dialog:\n",
                "                topic_dialog[topic] = {}\n",
                "            \n",
                "            dialogs = [utt for utt in line['dialogs'] if 'text' in utt]\n",
                "            dialogs = [utt for utt in dialogs if utt['text'] != '']\n",
                "            \n",
                "            topic_dialog[topic][dialog_language] = dialogs\n",
                "            if len(topic_dialog[topic]) == 2:\n",
                "                if '영어' in topic_dialog[topic]:\n",
                "                    task_ids = [3,5]\n",
                "                    prompt = input_prompt_text_en\n",
                "                    judge_model = model_en\n",
                "                    judge_tokenizer = tokenizer_en\n",
                "                elif '중국어' in topic_dialog[topic]:\n",
                "                    task_ids = [4,6]\n",
                "                    prompt = input_prompt_text_zh\n",
                "                    judge_model = model_zh\n",
                "                    judge_tokenizer = tokenizer_zh\n",
                "                    \n",
                "                en_kr_pairs = sorted(topic_dialog[topic].items())\n",
                "                topic_dialog.pop(topic)\n",
                "                if len(en_kr_pairs[0][1]) == len(en_kr_pairs[1][1]):\n",
                "                    judges = []\n",
                "                    sample_datas = []\n",
                "                    for src, tgt in zip(en_kr_pairs[1][1][:16], en_kr_pairs[0][1][:16]):\n",
                "                        data = {'input': None, 'output': None}\n",
                "                        data['input'] = src['text']\n",
                "                        data['output'] = tgt['text']\n",
                "                        sample_datas.append(data)\n",
                "                    input_samples = [input_prompt_text_en.format(**data) for data in sample_datas]        \n",
                "                    input_ids = judge_tokenizer(input_samples, return_tensors='pt', padding=True).to(judge_model.device)\n",
                "                    \n",
                "                    with torch.no_grad():\n",
                "                        outputs = judge_model.generate(**input_ids,\n",
                "                                                    max_new_tokens=1,\n",
                "                                                    pad_token_id=judge_tokenizer.eos_token_id)#[0][\"generated_text\"]\n",
                "                        \n",
                "                    \n",
                "                    judges = judge_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
                "                    judges_only = [judge.split('Judge:')[-1].strip() for judge in judges]\n",
                "                    # print(topic, judges)\n",
                "                    \n",
                "                    if sum([judge == 'No' for judge in judges_only]) > 8:\n",
                "                        print(source_data.name, topic)\n",
                "                        #print(judges_only)\n",
                "                        continue\n",
                "                    \n",
                "                    for utt_en, utt_kr in zip(en_kr_pairs[0][1], en_kr_pairs[1][1]):\n",
                "                        if bool(regex_pattern.findall(utt_kr['text'])) != bool(regex_pattern.findall(utt_en['text'])):\n",
                "                            continue\n",
                "                        #### 번역(한-영), 번역(한-중)\n",
                "                        data = {'input': None, 'output': None}\n",
                "                        data['input'] = utt_kr['text']\n",
                "                        data['output'] = utt_en['text']\n",
                "                        task_files[task_ids[0]].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                        # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "                        #### 번역(한-영), 번역(한-중) end\n",
                "                    \n",
                "                    \n",
                "                        #### 번역(영-한), 번역(중-한)\n",
                "                        data['input'], data['output'] = data['output'], data['input']\n",
                "                        task_files[task_ids[1]].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                        # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "                        #### 번역(영-한), 번역(중-한) end\n",
                "                    \n",
                "            # break\n",
                "        # break\n",
                "    # break\n",
                "\n",
                "    for path in task_files:\n",
                "        path.close()      \n",
                "        \n",
                "#### task preprocess end      "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "exit(0)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "undefined.undefined.undefined"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
