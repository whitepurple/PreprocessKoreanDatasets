{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "train: 100%|██████████| 13/13 [00:27<00:00,  2.11s/it]\n",
                        "valid: 100%|██████████| 16/16 [00:03<00:00,  4.87it/s]\n"
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "import re\n",
                "from tqdm import tqdm\n",
                "\n",
                "settings = json.loads(Path('../../../settings.json').read_text())",
                "preprocessed_data_path = Path(settings['preprocessed_data_path'])",
                "data_path = Path('.').resolve()",
                "data_name = data_path.name",
                "source_name = data_path.parent.name",
                "preprocessed_dir = preprocessed_data_path/source_name/data_name",

                "source_data_dir = preprocessed_dir/'preprocessed'\n",
                "splits = ['train', 'valid']\n",
                "tasks = ['LM(한국어)', '유해 질의 식별', '문서 내용 QA']\n",
                "task_data_dir = preprocessed_dir/'preprocessed_task'\n",
                "task_data_dir.mkdir(exist_ok=True)\n",
                "for task in tasks:\n",
                "    task_path = task_data_dir/task\n",
                "    task_path.mkdir(exist_ok=True)\n",
                "    \n",
                "#### prepare for task preprocess\n",
                "label_level_map = {\n",
                "    1: \"비난/혐오/차별\",\n",
                "        101: \"인종 및 민족\",\n",
                "        102: \"성별\",\n",
                "        103: \"성적지향\",\n",
                "        104: \"종교\",\n",
                "        105: \"나이\",\n",
                "        106: \"신체 및 정신 장애\",\n",
                "        107: \"체형 및 외모\",\n",
                "        108: \"지역 및 사회계층\",\n",
                "        109: \"직업 및 업무수행\",\n",
                "        110: \"정치적/사회적 신념\",\n",
                "    2: \"선정\",\n",
                "        201: \"경험\",\n",
                "        202: \"행동\",\n",
                "        203: \"매체\",\n",
                "    3: \"욕설\",\n",
                "        301: \"인신공격\",\n",
                "        302: \"비속어\",\n",
                "        303: \"모욕적 발언\",\n",
                "    4: \"폭력\",\n",
                "        401: \"정서적 폭력\",\n",
                "        402: \"사이버 폭력\",\n",
                "    5: \"범죄\",\n",
                "        501: \"폭력\",\n",
                "        502: \"재산\",\n",
                "        503: \"경제\",\n",
                "        504: \"공무원\",\n",
                "        505: \"마약\",\n",
                "        506: \"사이버\",\n",
                "        507: \"개인정보 침해\",\n",
                "    6: \"허위정보및루머\",\n",
                "        601: \"조작된 내용(연예인,정치인,집단등)가짜뉴스\",\n",
                "        602: \"편견이 심한 뉴스\",\n",
                "    7: \"스팸 및 광고\",\n",
                "        701: \"제품/서비스/이벤트 광고\",\n",
                "        702: \"정치광고\",\n",
                "        703: \"상업적 스팸\",\n",
                "        704: \"대량 메일 스팸\",\n",
                "        705: \"낚시 스팸\",\n",
                "        706: \"악성코드 스팸\"\n",
                "}\n",
                "persona_map = {\n",
                "    1: '존댓말+문어체',\n",
                "    2: '존댓말+구어체',\n",
                "    3: '반말+문어체',\n",
                "    4: '반말+구어체'\n",
                "}\n",
                "\n",
                "#### prepare for task preprocess end\n",
                "\n",
                "#### task preprocess\n",
                "for split in splits:\n",
                "    source_data_dir_split = source_data_dir/split\n",
                "    task_files = [(task_data_dir/task/f'{split}.jsonl').open('w', encoding='utf-8') for task in tasks]\n",
                "    for source_data in tqdm(list(source_data_dir_split.iterdir()), desc=split):\n",
                "        if source_data.stat().st_size == 0:\n",
                "            continue\n",
                "        data_name = source_data.stem\n",
                "        source_data = source_data.open()\n",
                "\n",
                "        if 'S_1. 말뭉치' in data_name:\n",
                "            for line in source_data.readlines():\n",
                "                line = json.loads(line)\n",
                "                title = line['title']\n",
                "                publisher_company = line['publisher_company']\n",
                "                category_main = line['category_main']\n",
                "                category_middle = line['category_middle']\n",
                "                collection_name = line['collection_name']\n",
                "                issue_date = line['issue_date']\n",
                "                info = '/'.join([publisher_company, category_main, category_middle, collection_name, issue_date])\n",
                "                \n",
                "                text = line['corpus']\n",
                "                #### LM(한국어)\n",
                "                data = {'text': None}\n",
                "                ## preprocess data from line\n",
                "                data['text'] = f'title: {title}\\ninfo: {info}\\ncontent: {text}'\n",
                "                ## preprocess data from line end\n",
                "                task_files[0].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "                #### LM(한국어) end\n",
                "        \n",
                "        if '_2. 유해질의' in data_name:\n",
                "            for line in source_data.readlines():\n",
                "                line = json.loads(line)\n",
                "                #### 유해 질의 식별\n",
                "                data = {'text': None, 'label': None}\n",
                "                ## preprocess data from line\n",
                "                data['text'] = line['instruct_text']\n",
                "                labels = line['labels'][0]\n",
                "                label = {\n",
                "                    'level1_type' : label_level_map[labels['level1_type']],\n",
                "                    'level2_type' : label_level_map[labels['level2_type']],\n",
                "                    'persona' : persona_map[labels['persona']]\n",
                "                }\n",
                "                data['label'] = label\n",
                "                ## preprocess data from line end\n",
                "                task_files[1].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "                #### 유해 질의 식별 end\n",
                "        \n",
                "        if 'L_1. 질의응답' in data_name or 'L_1. 말뭉치' in data_name:\n",
                "            for line in source_data.readlines():\n",
                "                line = json.loads(line)\n",
                "                publisher = line['publisher']\n",
                "                category = line['type']\n",
                "                date = line['date']\n",
                "                title = line['title']\n",
                "                context = line['context']\n",
                "                labels = line['labels']\n",
                "                for label in labels:\n",
                "                    instructions = [i['text'] for i in label['instructs']]\n",
                "                    response = label['response']\n",
                "                    #### 문서 내용 QA\n",
                "                    data = {'input': {}, 'output': None}\n",
                "                    ## preprocess data from line\n",
                "                    data['input']['publisher'] = publisher\n",
                "                    data['input']['date'] = date\n",
                "                    data['input']['type'] = category\n",
                "                    data['input']['title'] = title\n",
                "                    data['input']['context'] = context\n",
                "                    data['input']['instructions'] = instructions\n",
                "                    data['output'] = response\n",
                "                    ## preprocess data from line end\n",
                "                    task_files[2].write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
                "                    # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
                "                    #### 문서 내용 QA end\n",
                "\n",
                "    for path in task_files:\n",
                "        path.close()      \n",
                "        \n",
                "#### task preprocess end      "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
